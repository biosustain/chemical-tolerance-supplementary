{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook analyses the coverage across genes of the sequenced strains. Coverages are normalised by gene and a duplication-calling cutoff is set as 1.5 times the mode of coverage across strains in the given gene.\n",
    "\n",
    "To get more contiguous duplications a segmentation procedure was applied to the called duplications. For this purpose the Viterbi algorithm for finding the most likely sequence of states in a Hidden Markov Model was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genome = SeqIO.read(\"../Data/Mutation_data/MG1655.gb\", \"genbank\")\n",
    "\n",
    "position_dict = {}\n",
    "for feat in genome.features:\n",
    "    if \"locus_tag\" in feat.qualifiers:\n",
    "        position_dict[feat.qualifiers[\"locus_tag\"][0]] = feat.location.start.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coverage = pd.read_csv(\"../Data/Mutation_data/coverage_FC_by_gene.tab.tsv\", sep=\" \", index_col=0)\n",
    "names = coverage.columns\n",
    "fixed_names = []\n",
    "for name in names:\n",
    "    ale = name.split(\"_\")[0]\n",
    "    name = name.split(\"_\")[-3]\n",
    "    if name.startswith(\"ALE-\"):\n",
    "        name = name[4:]\n",
    "    name = name.replace(\"HDMA\", \"HMDA\")\n",
    "    name = name.replace(\"Glut-\", \"GLUT\")\n",
    "    name = name.replace(\"PUTR-\", \"PUTR\")\n",
    "    fixed_names.append(ale + \"_\" + name)\n",
    "#coverage.columns = fixed_names\n",
    "coverage.to_csv(\"../Data/Mutation_data/cleaned_coverage_by_gene.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalise coverage by gene (divide by gene median)\n",
    "norm_coverage = (coverage.T / coverage.T.median(0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_mode(nums, binsize=0.005):\n",
    "    \"\"\"Find the mode of a distribution.\"\"\"\n",
    "    min_val = nums.min()\n",
    "    max_val = nums.max()\n",
    "    bins = np.array([0 for i in range(int((max_val - min_val) // binsize) + 1)])\n",
    "    for num in nums:\n",
    "        bin_num = int((num - min_val) // binsize)\n",
    "        bins[bin_num] += 1\n",
    "    return bins.argmax() * binsize + min_val\n",
    "\n",
    "def cutoff_coverages(ser):\n",
    "    \"\"\"Convert a series to binary, denoting whether each point is greater or less than the mode of the series\"\"\"\n",
    "    peak = find_mode(ser)\n",
    "    return (ser > peak * 1.5).astype(\"int\")\n",
    "\n",
    "def segment_series(ser):\n",
    "    \"\"\"Segment a binary series using the Viterbi algorithm to find the most likely trajectory.\"\"\"\n",
    "    if ser.name not in duplications:\n",
    "        print(ser.name)\n",
    "        return ser\n",
    "    peak = find_mode(ser.astype(\"int\"))\n",
    "    obs = np.array([stats.norm(peak, 0.2), stats.norm(2*peak, 0.2)])\n",
    "    decoder = Decoder(start_probability, trans_mat, obs)\n",
    "    return decoder.Decode(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cutoff normalised coverage\n",
    "duplications = norm_coverage.apply(cutoff_coverages)\n",
    "\n",
    "\n",
    "dup_rate = duplications.sum().sum() / (duplications.shape[0] * duplications.shape[1])\n",
    "transition_rate = 0.001 / 11\n",
    "\n",
    "trans_mat = np.array([[1-transition_rate, transition_rate], [transition_rate, 1-transition_rate]])\n",
    "start_probability = np.array([[1 - dup_rate, dup_rate]]).T\n",
    "\n",
    "false_positive = 0.01\n",
    "false_negative = 0.5\n",
    "obs_mat = np.array([[1-false_positive, false_positive],\n",
    "                    [false_negative, 1-false_negative]])\n",
    "obs_mat = obs_mat * 1.2 # Scale to avoid numerical underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the genome positions of each gene\n",
    "duplications[\"pos\"] = duplications.index.map(position_dict.get) \n",
    "\n",
    "# Calculate \"length\" of each gene (distance to next gene, measured in 100 bp's)\n",
    "duplications[\"length\"] = (\n",
    "    pd.Series(list(duplications[\"pos\"][1:]) + [len(genome)], index=duplications.index) - duplications[\"pos\"]\n",
    ").fillna(0) // 100 + 1\n",
    "duplications[\"length\"] = duplications[\"length\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Expand genes according to their length\n",
    "data = []\n",
    "for idx, row in duplications.iterrows():\n",
    "    for i in range(int(row[\"length\"])):\n",
    "        row_dict = dict(row)\n",
    "        row_dict[\"b_num\"] = idx\n",
    "        row_dict[\"num\"] = i\n",
    "        data.append(row_dict)\n",
    "long_duplications = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Viterbi code from https://github.com/phvu/misc/blob/master/viterbi/viterbi.py\n",
    "\n",
    "class Decoder(object):\n",
    "    def __init__(self, initialProb, transProb, obsProb):\n",
    "        self.N = initialProb.shape[0]\n",
    "        self.initialProb = initialProb\n",
    "        self.transProb = transProb\n",
    "        self.obsProb = obsProb\n",
    "        assert self.initialProb.shape == (self.N, 1)\n",
    "        assert self.transProb.shape == (self.N, self.N)\n",
    "        assert self.obsProb.shape[0] == self.N\n",
    "        \n",
    "    def Obs(self, obs, n=None):\n",
    "        o = self.obsProb[:, obs, None]\n",
    "        # if n is not None:\n",
    "        #     o = self.obs_probabilities[:, n, None]\n",
    "        # else:\n",
    "        #     o = np.array([[p.pdf(obs)] for p in self.obsProb])\n",
    "        # print(repr(o))\n",
    "        return o\n",
    "\n",
    "    def Decode(self, obs):\n",
    "        #self.obs_probabilities = np.array([p.pdf(obs) for p in self.obsProb])\n",
    "        global trellis\n",
    "        trellis = np.zeros((self.N, len(obs)), \"float128\")\n",
    "        backpt = np.ones((self.N, len(obs)), 'int32') * -1\n",
    "                \n",
    "        # initialization\n",
    "        trellis[:, 0] = np.squeeze(self.initialProb * self.Obs(obs[0]))\n",
    "                \n",
    "        for t in range(1, len(obs)):\n",
    "            trellis[:, t] = (trellis[:, t-1, None].dot(self.Obs(obs[t], t).T) * self.transProb).max(0)\n",
    "            backpt[:, t] = (np.tile(trellis[:, t-1, None], [1, self.N]) * self.transProb).argmax(0)\n",
    "        # termination\n",
    "        tokens = [trellis[:, -1].argmax()]\n",
    "        for i in range(len(obs)-1, 0, -1):\n",
    "            tokens.append(backpt[tokens[-1], i])\n",
    "        return tokens[::-1]\n",
    "    \n",
    "decoder = Decoder(start_probability, trans_mat, obs_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "long_segmented_duplications = long_duplications.copy()\n",
    "\n",
    "#results = {}\n",
    "#with ProcessPoolExecutor() as executor:\n",
    "#    # Add each segmentation task to the process pool\n",
    "#    for n in duplications:\n",
    "#        if n in [\"pos\", \"length\"]:\n",
    "#            continue\n",
    "#        results[n] = executor.submit(decoder.Decode, long_duplications[n].astype(\"int\"))\n",
    "#    \n",
    "#    print(\"Started all tasks...\")\n",
    "#    # Get results\n",
    "#    for n in duplications:\n",
    "#        if n in [\"pos\", \"length\"]:\n",
    "#            continue\n",
    "#        long_segmented_duplications[n] = results[n].result()\n",
    "        \n",
    "#\n",
    "# To run serially without the ProcessPoolExecutor, comment the code above and uncomment the code below:\n",
    "#\n",
    "\n",
    "for n in duplications:\n",
    "    if n in [\"pos\", \"length\"]:\n",
    "        continue\n",
    "    long_segmented_duplications[n] = decoder.Decode(long_duplications[n].astype(\"int\"))\n",
    "    \n",
    "segmented_duplications = long_segmented_duplications.groupby(\"b_num\").mean()[list(coverage.columns)].astype(\"int\")\n",
    "segmented_duplications = segmented_duplications.reindex(list(duplications.index)) # Restore original ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_duplications = coverage.copy()\n",
    "for col in list(coverage):\n",
    "    merged_duplications[col] = duplications[col] | segmented_duplications[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_duplications.to_csv(\"../Data/Mutation_data/Overlaid_segmented_and_raw_duplications.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_duplications(ser):\n",
    "    ser = pd.concat([ser, ser])\n",
    "    flag = 0\n",
    "    start = None\n",
    "    i = 0\n",
    "    while True:\n",
    "        if ser[i] == 0:\n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "    for idx, s in ser[i:].items():\n",
    "        if s == 1:\n",
    "            if flag == 0:\n",
    "                flag = 1\n",
    "                start = idx\n",
    "        elif s == 0:\n",
    "            if flag == 1:\n",
    "                flag = 0\n",
    "                yield start, last_idx\n",
    "        last_idx = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "data = OrderedDict()\n",
    "for c in coverage:\n",
    "    for start, end in parse_duplications(merged_duplications[c]):\n",
    "        dat = (c, start, end)\n",
    "        if dat not in data:\n",
    "            data[dat] = 1\n",
    "            \n",
    "duplication_calls = pd.DataFrame(list(data.keys()), columns=[\"Strain\", \"first_gene\", \"last_gene\"])\n",
    "duplication_calls.to_csv(\"../Data/Mutation_data/duplication_calls.tsv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tolerance]",
   "language": "python",
   "name": "Python [tolerance]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
